{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2464,
     "status": "ok",
     "timestamp": 1646834483413,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "4iIkazdQLFX1",
    "outputId": "4afde7df-0330-4491-f27a-fcdd5ef4aeae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNLnBKtsxEDu"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjSocV36vR5H"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# import codecs\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "# from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import GlobalAveragePooling1D\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fApaD4W5xBkm"
   },
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1646834486143,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "H8qB_Z2cviqm",
    "outputId": "8a755cd4-b8f8-4208-f27b-d291839ba9ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_185_135_0.32_0.37\n"
     ]
    }
   ],
   "source": [
    "PATH = '/content/drive/MyDrive/Colab Notebooks/skillfactory/Project-Final/'\n",
    "PATH_DATA = PATH + 'data/'\n",
    "embedding_file = PATH_DATA + 'glove.840B.300d.txt'\n",
    "train_data_file = PATH_DATA + 'train.csv'\n",
    "test_data_file = PATH_DATA + 'test.csv'\n",
    "max_sequence_length = 60\n",
    "max_num_words = 200_000 # There are about 201000 unique words in training dataset, 200000 is enough for tokenization\n",
    "embedding_dim = 300\n",
    "validation_split_ratio = 0.1\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "lstm_name = 'lstm_{:d}_{:d}_{:.2f}_{:.2f}'.format(num_lstm, num_dense, rate_drop_lstm, rate_drop_dense)\n",
    "print(lstm_name)\n",
    "\n",
    "act_f = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.4% share in test set\n",
    "np.random.seed(21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5BynRAcxRUc"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUbaeSYXxUBB"
   },
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # Remove punctuation from text\n",
    "    # text = \"\".join([c for c in text if c not in punctuation])\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    # text = re.sub(r\"\\0s\", \"0\", text) # It doesn't make sense to me\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeZO49Lmw8YP"
   },
   "source": [
    "# Create word embedding dictionary\n",
    "from 'glove.840B.300d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 124314,
     "status": "ok",
     "timestamp": 1646834610445,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "B_wnfnq_wrwn",
    "outputId": "546b694c-9650-4594-f29e-e612bcb55e6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create word embedding dictionary\n",
      "Found 2195892 word vectors of glove.\n"
     ]
    }
   ],
   "source": [
    "print('Create word embedding dictionary')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open(embedding_file, encoding='utf-8')\n",
    "\n",
    "# for line in tqdm(f):\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    # word = values[0]\n",
    "    word = ''.join(values[:-300])   \n",
    "    # coefs = np.asarray(values[1:], dtype='float32')\n",
    "    coefs = np.asarray(values[-300:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f'Found {len(embeddings_index)} word vectors of glove.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrw44v_dw4xR"
   },
   "source": [
    "# Process text in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1077,
     "status": "ok",
     "timestamp": 1646834611515,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "8DoH1UdGxO2C",
    "outputId": "ae73d148-f9f7-4302-a105-b9fc9083b431"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text in dataset\n"
     ]
    }
   ],
   "source": [
    "print('Processing text in dataset')\n",
    "\n",
    "# load data and process with text_to_wordlist\n",
    "train_texts_1 = [] \n",
    "train_texts_2 = []\n",
    "train_labels = []\n",
    "\n",
    "df_train = pd.read_csv(train_data_file, encoding='utf-8')\n",
    "# df_train = df_train.sample(5000) # train data sample to test code\n",
    "df_train = df_train.fillna('empty')\n",
    "train_q1 = df_train['question1'].values\n",
    "train_q2 = df_train['question2'].values\n",
    "train_labels = df_train['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25883,
     "status": "ok",
     "timestamp": 1646834637393,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "1bOzT1w1yTpp",
    "outputId": "05849481-3d93-49e0-cc09-01d8ed1c5df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404290 texts are found in train.csv\n"
     ]
    }
   ],
   "source": [
    "for text in train_q1:\n",
    "    train_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in train_q2:\n",
    "    train_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "\n",
    "'''\n",
    "with open(Train_Data_File, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader) # Skip header row\n",
    "    for values in reader:\n",
    "        train_texts_1.append(text_to_wordlist(values[3], remove_stopwords=False, stem_words=False))\n",
    "        train_texts_2.append(text_to_wordlist(values[4], remove_stopwords=False, stem_words=False))\n",
    "        train_labels.append(int(values[5]))\n",
    "'''\n",
    "print(f'{len(train_texts_1)} texts are found in train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103
    },
    "executionInfo": {
     "elapsed": 8878,
     "status": "ok",
     "timestamp": 1646834646254,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "5F9qsW8iyugB",
    "outputId": "df4c5ed5-a053-42e1-f7e3-9dd77c5a1253"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nwith open(Test_Data_File, encoding='utf-8') as f:\\n    reader = csv.reader(f, delimiter=',')\\n    header = next(reader)\\n    for values in reader:\\n        test_texts_1.append(text_to_wordlist(values[1], remove_stopwords=False, stem_words=False))\\n        test_texts_2.append(text_to_wordlist(values[2], remove_stopwords=False, stem_words=False))\\n        test_ids.append(values[0])\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "\n",
    "df_test = pd.read_csv(test_data_file, encoding='utf-8')\n",
    "# df_test = df_test.sample(5000) # test data sample to test code\n",
    "df_test = df_test.fillna('empty')\n",
    "test_q1 = df_test['question1'].values\n",
    "test_q2 = df_test['question2'].values\n",
    "test_ids = df_test['test_id'].values\n",
    "\n",
    "'''\n",
    "with open(Test_Data_File, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1], remove_stopwords=False, stem_words=False))\n",
    "        test_texts_2.append(text_to_wordlist(values[2], remove_stopwords=False, stem_words=False))\n",
    "        test_ids.append(values[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 228694,
     "status": "ok",
     "timestamp": 1646834874939,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "uYcq_Nq-y9pw",
    "outputId": "d79cacf5-2334-4b0a-cb5c-ddf00bbdc272"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3563475 texts are found in test.csv\n"
     ]
    }
   ],
   "source": [
    "for text in test_q1:\n",
    "    test_texts_1.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "    \n",
    "for text in test_q2:\n",
    "    test_texts_2.append(text_to_wordlist(text, remove_stopwords=False, stem_words=False))\n",
    "\n",
    "print(f'{len(test_texts_1)} texts are found in test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdUtXZP61t9K"
   },
   "source": [
    "# Tokenize words in all sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205526,
     "status": "ok",
     "timestamp": 1646835080454,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "5iGQeCMj1smb",
    "outputId": "ab87fbe4-a1e8-4b7f-db30-465c7394a771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120499 unique tokens are found\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_num_words)\n",
    "tokenizer.fit_on_texts(train_texts_1 + train_texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "train_sequences_1 = tokenizer.texts_to_sequences(train_texts_1)\n",
    "train_sequences_2 = tokenizer.texts_to_sequences(train_texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(f'{len(word_index)} unique tokens are found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21472,
     "status": "ok",
     "timestamp": 1646835101907,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "aZZrb7Xd2AZy",
    "outputId": "59c03fa3-7617-4a1d-b185-f1f188e33c8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train data tensor: (404290, 60)\n",
      "Shape of train labels tensor: (404290,)\n",
      "Shape of test data tensor: (3563475, 60)\n",
      "Shape of test ids tensor:(3563475,)\n"
     ]
    }
   ],
   "source": [
    "# pad all train with Max_Sequence_Length\n",
    "train_data_1 = pad_sequences(train_sequences_1, maxlen=max_sequence_length)\n",
    "train_data_2 = pad_sequences(train_sequences_2, maxlen=max_sequence_length)\n",
    "# train_labels = np.array(train_labels)\n",
    "print(f'Shape of train data tensor: {train_data_1.shape}')\n",
    "print(f'Shape of train labels tensor: {train_labels.shape}')\n",
    "\n",
    "# pad all test with Max_Sequence_Length\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=max_sequence_length)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=max_sequence_length)\n",
    "# test_ids = np.array(test_ids)\n",
    "print(f'Shape of test data tensor: {test_data_2.shape}')\n",
    "print(f'Shape of test ids tensor:{test_ids.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7KDNQC82rur"
   },
   "source": [
    "# Leaky features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUvpTbOm2rFH"
   },
   "outputs": [],
   "source": [
    "questions = pd.concat([df_train[['question1', 'question2']], \\\n",
    "                        df_test[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(questions.shape[0]):\n",
    "    q_dict[questions.question1[i]].add(questions.question2[i])\n",
    "    q_dict[questions.question2[i]].add(questions.question1[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yH3gZ96b26hk"
   },
   "outputs": [],
   "source": [
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def q1_freq(row):\n",
    "    return(len(q_dict[row['question1']]))\n",
    "    \n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row['question2']]))\n",
    "\n",
    "df_train['q1_q2_intersect'] = df_train.apply(q1_q2_intersect, axis=1)\n",
    "df_train['q1_freq'] = df_train.apply(q1_freq, axis=1)\n",
    "df_train['q2_freq'] = df_train.apply(q2_freq, axis=1)\n",
    "\n",
    "df_test['q1_q2_intersect'] = df_test.apply(q1_q2_intersect, axis=1)\n",
    "df_test['q1_freq'] = df_test.apply(q1_freq, axis=1)\n",
    "df_test['q2_freq'] = df_test.apply(q2_freq, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgc_wjdd3ylO"
   },
   "outputs": [],
   "source": [
    "leaks = df_train[['q1_q2_intersect', 'q1_freq', 'q2_freq']]\n",
    "test_leaks = df_test[['q1_q2_intersect', 'q1_freq', 'q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1646835355174,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "qf7BiUae4A67",
    "outputId": "b11b8891-c0b1-4848-b355-0f7f2feb5b80"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:444: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((leaks, test_leaks)))\n",
    "leaks = ss.transform(leaks)\n",
    "test_leaks = ss.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w44x9PuC4E_-"
   },
   "source": [
    "# Create embedding matrix for embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 506,
     "status": "ok",
     "timestamp": 1646835355672,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "HGBjtqyj4R3c",
    "outputId": "68d9913b-1127-4206-aa58-647d6430c699"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 33233\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix')\n",
    "num_words = min(max_num_words, len(word_index))+1\n",
    "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print(f'Null word embeddings: {np.sum(np.sum(embedding_matrix, axis=1) == 0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctWXFfiv5BRd"
   },
   "source": [
    "# Train Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3X3w6VmA5C1u"
   },
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(train_data_1))\n",
    "idx_train = perm[:int(len(train_data_1)*(1-validation_split_ratio))]\n",
    "idx_val = perm[int(len(train_data_1)*(1-validation_split_ratio)):]\n",
    "\n",
    "data_1_train = np.vstack((train_data_1[idx_train], train_data_2[idx_train]))\n",
    "data_2_train = np.vstack((train_data_2[idx_train], train_data_1[idx_train]))\n",
    "leaks_train = np.vstack((leaks[idx_train], leaks[idx_train]))\n",
    "labels_train = np.concatenate((train_labels[idx_train], train_labels[idx_train]))\n",
    "\n",
    "data_1_val = np.vstack((train_data_1[idx_val], train_data_2[idx_val]))\n",
    "data_2_val = np.vstack((train_data_2[idx_val], train_data_1[idx_val]))\n",
    "leaks_val = np.vstack((leaks[idx_val], leaks[idx_val]))\n",
    "labels_val = np.concatenate((train_labels[idx_val], train_labels[idx_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2TkDr2S5PWl"
   },
   "outputs": [],
   "source": [
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.471544715\n",
    "    weight_val[labels_val==0] = 1.309033281"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um3FjzgG5Qtf"
   },
   "source": [
    "# The embedding layer containing the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3127,
     "status": "ok",
     "timestamp": 1646835359727,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "chH6h3G655D_",
    "outputId": "fffea440-11a3-4319-d75e-e1a252ee9f53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "emb_layer = Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=Max_Sequence_Length,\n",
    "    trainable=False\n",
    ")\n",
    "'''\n",
    "\n",
    "emb_layer = Embedding(\n",
    "    input_dim=num_words,\n",
    "    output_dim=embedding_dim,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=max_sequence_length,\n",
    "    trainable=False\n",
    ")    \n",
    "\n",
    "# LSTM layer\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "# Define inputs\n",
    "seq1 = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "seq2 = Input(shape=(max_sequence_length,), dtype='int32')\n",
    "\n",
    "# Run inputs through embedding\n",
    "emb1 = emb_layer(seq1)\n",
    "emb2 = emb_layer(seq2)\n",
    "\n",
    "# Run through LSTM layers\n",
    "lstm_a = lstm_layer(emb1)\n",
    "# glob1a = GlobalAveragePooling1D()(lstm_a)\n",
    "lstm_b = lstm_layer(emb2)\n",
    "# glob1b = GlobalAveragePooling1D()(lstm_b)\n",
    "\n",
    "magic_input = Input(shape=(leaks.shape[1],))\n",
    "# magic_dense = BatchNormalization()(magic_input)\n",
    "magic_dense = Dense(int(num_dense/2), activation=act_f)(magic_input)\n",
    "\n",
    "merged = concatenate([lstm_a, lstm_b, magic_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act_f)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5xmMGnB562Z"
   },
   "outputs": [],
   "source": [
    "# Add class weight\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309033281, 1: 0.471544715}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "354B0xeB59RX"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9f_c__n6Qkf"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=[seq1, seq2, magic_input], outputs=preds)\n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UnsqTfe_60gG"
   },
   "outputs": [],
   "source": [
    "# Set early stopping (large patience should be useful)\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=10)\n",
    "bst_model_path = PATH + 'result/' + lstm_name + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4695639,
     "status": "ok",
     "timestamp": 1646840055346,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "NZ5WIybf7DVm",
    "outputId": "d24a1a90-9bdc-4705-a156-83b60d52d096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "356/356 [==============================] - 3627s 10s/step - loss: 0.2847 - acc: 0.8210 - val_loss: 0.2953 - val_acc: 0.7452\n",
      "Epoch 2/200\n",
      "356/356 [==============================] - 3612s 10s/step - loss: 0.2384 - acc: 0.8364 - val_loss: 0.2225 - val_acc: 0.8346\n",
      "Epoch 3/200\n",
      "177/356 [=============>................] - ETA: 29:42 - loss: 0.2277 - acc: 0.8399"
     ]
    }
   ],
   "source": [
    "hist = model.fit([data_1_train, data_2_train, leaks_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, leaks_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJoKPsOD7HWV"
   },
   "outputs": [],
   "source": [
    "model.load_weights(bst_model_path) # store model parameters in .h5 file\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f79ErzLm7JOg"
   },
   "source": [
    "# Make the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 356552,
     "status": "ok",
     "timestamp": 1646840414682,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "_cGt-AsQLC8f",
    "outputId": "430a8609-df8e-4bfc-e09e-6bd19b78c7e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "435/435 [==============================] - 178s 408ms/step\n",
      "435/435 [==============================] - 178s 409ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict([test_data_1, test_data_2, test_leaks], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_leaks], batch_size=8192, verbose=1)\n",
    "preds /= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 5611,
     "status": "ok",
     "timestamp": 1646842577178,
     "user": {
      "displayName": "Eugene Mukaseev",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15628526554498694407"
     },
     "user_tz": -180
    },
    "id": "0pI_be1d7RU2"
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'test_id': test_ids,\n",
    "                           'is_duplicate': preds.ravel()})\n",
    "submission[:2345796].to_csv(PATH+ 'result/{:.4f}_'.format(bst_val_score)+lstm_name+'_with_GloVe_Embedding.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JsiJY0PfgiHR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPpXT1QTNDUZgKcq0E8kYid",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "LSTM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
